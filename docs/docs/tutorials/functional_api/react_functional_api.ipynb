{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol_kGR2Erar2"
      },
      "outputs": [],
      "source": [
        "! pip install langchain_core langchain-anthropic langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZbHAWHjrllG",
        "outputId": "384c03cf-52a5-4eb7-f1f7-a384c30f689f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5gtH8KQrar4",
        "outputId": "75d3008d-f025-4043-e266-90994356ed43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANTHROPIC_API_KEY: ··········\n"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "\n",
        "_set_env(\"ANTHROPIC_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HlLF5cScrar5"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amp4sJrJrar5"
      },
      "source": [
        "### Vanilla Agent\n",
        "\n",
        "* No orchestration framework\n",
        "* Use LangChain to bind tools and specify tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0EJ1_mXXrar7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "# Define tools\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@tool\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "# Augment the LLM with tools\n",
        "tools = [add, multiply, divide]\n",
        "tools_by_name = {tool.name: tool for tool in tools}\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBW0Ku4Urar8",
        "outputId": "2183f7bb-663a-4490-8840-6dddc0c742e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 3 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "[{'text': \"I'll help you add 3 and 4 using the `add` function.\", 'type': 'text'}, {'id': 'toolu_014cfgAAKpiZPmzSd93Dx4q3', 'input': {'a': 3, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
            "Tool Calls:\n",
            "  add (toolu_014cfgAAKpiZPmzSd93Dx4q3)\n",
            " Call ID: toolu_014cfgAAKpiZPmzSd93Dx4q3\n",
            "  Args:\n",
            "    a: 3\n",
            "    b: 4\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The sum of 3 and 4 is 7.\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import add_messages\n",
        "from langchain_core.messages import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    BaseMessage,\n",
        "    ToolCall,\n",
        ")\n",
        "\n",
        "def call_llm(messages: list[BaseMessage]):\n",
        "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "    return llm_with_tools.invoke(\n",
        "        [\n",
        "            SystemMessage(\n",
        "                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
        "            )\n",
        "        ]\n",
        "        + messages\n",
        "    )\n",
        "\n",
        "def call_tool(tool_call: ToolCall):\n",
        "    \"\"\"Performs the tool call\"\"\"\n",
        "\n",
        "    tool = tools_by_name[tool_call[\"name\"]]\n",
        "    return tool.invoke(tool_call)\n",
        "\n",
        "def agent(messages: list[BaseMessage]):\n",
        "    \"\"\" Tool calling agent \"\"\"\n",
        "    llm_response = call_llm(messages)\n",
        "\n",
        "    while True:\n",
        "        if not llm_response.tool_calls:\n",
        "            break\n",
        "\n",
        "        # Execute tools\n",
        "        tool_results = [\n",
        "            call_tool(tool_call) for tool_call in llm_response.tool_calls\n",
        "        ]\n",
        "        messages = add_messages(messages, [llm_response, *tool_results])\n",
        "        llm_response = call_llm(messages)\n",
        "\n",
        "    messages = add_messages(messages, llm_response)\n",
        "    return messages\n",
        "\n",
        "# Stream\n",
        "messages = agent([HumanMessage(content=\"Add 3 and 4.\")])\n",
        "for m in messages:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHQwQO5srar9"
      },
      "source": [
        "### Agent with short-term memory (within a thread)\n",
        "\n",
        "* LangGraph persistence layer\n",
        "* Allows you to resume any conversation with the agent\n",
        "\n",
        "`@entrypoint`\n",
        "* Decorator indicates the start of a workflow / agent\n",
        "* Produces a Pregel object, an abstraction for managing a few things\n",
        "* Execution -- Syncronous (invoke), Async (ainvoke), streaming (stream)\n",
        "* State -- Checkpointing, Human in the loop (interrupt)\n",
        "\n",
        "[Optional: `@entrypoint.final`](https://langchain-ai.github.io/langgraph/concepts/functional_api/#entrypointfinal)\n",
        "* Can be used to specify what to return vs what to checkpoint\n",
        "\n",
        "`@task`\n",
        "* Results from tasks are saved as checkpoints\n",
        "* Important for caching results (time-consuming operations)\n",
        "* Support streaming updates from tasks\n",
        "* Support tracing\n",
        "\n",
        "Calling a task --\n",
        "* When you call a task, it returns immediately with a future object.\n",
        "* A future is a placeholder for a result that will be available later.\n",
        "* `.result()` marks where in the code you actually need the task's result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zpAXRlI2rar-"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from langgraph.func import entrypoint, task # New\n",
        "from langgraph.checkpoint.memory import MemorySaver # New\n",
        "\n",
        "@task # New\n",
        "def call_llm(messages: list[BaseMessage]):\n",
        "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "    return llm_with_tools.invoke(\n",
        "        [\n",
        "            SystemMessage(\n",
        "                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
        "            )\n",
        "        ]\n",
        "        + messages\n",
        "    )\n",
        "\n",
        "@task # New\n",
        "def call_tool(tool_call: ToolCall):\n",
        "    \"\"\"Performs the tool call\"\"\"\n",
        "\n",
        "    tool = tools_by_name[tool_call[\"name\"]]\n",
        "    return tool.invoke(tool_call)\n",
        "\n",
        "checkpointer = MemorySaver() # New\n",
        "@entrypoint(checkpointer=checkpointer) # New\n",
        "def agent(messages: list[BaseMessage], previous: list[BaseMessage]): # previous (state associated with the previous ckpt)\n",
        "    \"\"\" Tool calling agent \"\"\"\n",
        "\n",
        "    # Add previous messages from short-term memory to the current messages\n",
        "    if previous is not None:\n",
        "        messages = add_messages(previous, messages)\n",
        "\n",
        "    # Call the LLM\n",
        "    llm_response = call_llm(messages).result()\n",
        "\n",
        "    while True:\n",
        "        if not llm_response.tool_calls:\n",
        "            break\n",
        "\n",
        "        # Execute tools\n",
        "        tool_results = [\n",
        "            call_tool(tool_call).result() for tool_call in llm_response.tool_calls\n",
        "        ]\n",
        "        messages = add_messages(messages, [llm_response, *tool_results])\n",
        "        llm_response = call_llm(messages).result()\n",
        "\n",
        "    messages = add_messages(messages, llm_response)\n",
        "\n",
        "    # Return LLM response and save the full message history\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCb_P7Urrar-",
        "outputId": "39e6111a-75f9-4590-e557-38610a92ebfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 4 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "[{'text': \"I'll help you add 4 and 4 using the add function.\", 'type': 'text'}, {'id': 'toolu_01DCxHTmk9jqUi5RTtgHBtvD', 'input': {'a': 4, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
            "Tool Calls:\n",
            "  add (toolu_01DCxHTmk9jqUi5RTtgHBtvD)\n",
            " Call ID: toolu_01DCxHTmk9jqUi5RTtgHBtvD\n",
            "  Args:\n",
            "    a: 4\n",
            "    b: 4\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "8\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The sum of 4 and 4 is 8.\n"
          ]
        }
      ],
      "source": [
        "# Thread ID\n",
        "thread_id = str(uuid.uuid4())\n",
        "\n",
        "# Config\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "# Run with checkpointer to persist state in memory\n",
        "messages = agent.invoke([HumanMessage(content=\"Add 4 and 4.\")], config)\n",
        "for m in messages:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qWVWZRvrar_",
        "outputId": "6bdb71ae-5fd4-49c8-c0cc-7e5ce841bd18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 4 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "[{'text': \"I'll help you add 4 and 4 using the add function.\", 'type': 'text'}, {'id': 'toolu_01DCxHTmk9jqUi5RTtgHBtvD', 'input': {'a': 4, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
            "Tool Calls:\n",
            "  add (toolu_01DCxHTmk9jqUi5RTtgHBtvD)\n",
            " Call ID: toolu_01DCxHTmk9jqUi5RTtgHBtvD\n",
            "  Args:\n",
            "    a: 4\n",
            "    b: 4\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "8\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The sum of 4 and 4 is 8.\n"
          ]
        }
      ],
      "source": [
        "# Get the last checkpoint, which contains the full message history\n",
        "agent_state = agent.get_state(config)\n",
        "for m in agent_state.values:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx1dJODIrar_",
        "outputId": "4bb7e014-7994-4d94-e073-0e82535ff097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 4 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "[{'text': \"I'll help you add 4 and 4 using the add function.\", 'type': 'text'}, {'id': 'toolu_01DCxHTmk9jqUi5RTtgHBtvD', 'input': {'a': 4, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
            "Tool Calls:\n",
            "  add (toolu_01DCxHTmk9jqUi5RTtgHBtvD)\n",
            " Call ID: toolu_01DCxHTmk9jqUi5RTtgHBtvD\n",
            "  Args:\n",
            "    a: 4\n",
            "    b: 4\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "8\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The sum of 4 and 4 is 8.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Take the result and multiply it by 2.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "[{'text': \"I'll take the previous result (8) and multiply it by 2 using the multiply function.\", 'type': 'text'}, {'id': 'toolu_019Z9mXDjQnQLSpcbYDuWR47', 'input': {'a': 8, 'b': 2}, 'name': 'multiply', 'type': 'tool_use'}]\n",
            "Tool Calls:\n",
            "  multiply (toolu_019Z9mXDjQnQLSpcbYDuWR47)\n",
            " Call ID: toolu_019Z9mXDjQnQLSpcbYDuWR47\n",
            "  Args:\n",
            "    a: 8\n",
            "    b: 2\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "16\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "8 multiplied by 2 equals 16.\n"
          ]
        }
      ],
      "source": [
        "# Continue with the same thread\n",
        "messages = agent.invoke([HumanMessage(content=\"Take the result and multiply it by 2.\")], config)\n",
        "for m in messages:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FUkx6ezrasA"
      },
      "source": [
        "### Agent with HITL\n",
        "\n",
        "* Very useful for [approval](https://www.anthropic.com/research/building-effective-agents) in agents.\n",
        "* Add interrupt to the workflow to allow for HITL.\n",
        "* Re-executes from the start of the entrypoint.\n",
        "* Output of each `@task` is cached / saved as a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XSbhBHPXrasA"
      },
      "outputs": [],
      "source": [
        "from langgraph.types import interrupt\n",
        "\n",
        "@task\n",
        "def call_llm(messages: list[BaseMessage]):\n",
        "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "    print(\"Calling LLM!\")\n",
        "    return llm_with_tools.invoke(\n",
        "        [\n",
        "            SystemMessage(\n",
        "                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
        "            )\n",
        "        ]\n",
        "        + messages\n",
        "    )\n",
        "\n",
        "@task\n",
        "def call_tool(tool_call: ToolCall):\n",
        "    \"\"\"Performs the tool call\"\"\"\n",
        "    print(\"Calling tool!\")\n",
        "    # Interrupt the workflow to get a review from a human.\n",
        "    is_approved = interrupt({ # New\n",
        "            # Any json-serializable payload provided to interrupt as argument.\n",
        "            # It will be surfaced on the client side as an Interrupt when streaming data\n",
        "            # from the workflow.\n",
        "            \"tool_call\": tool_call, # The tool call we want reviewed.\n",
        "            # We can add any additional information that we need.\n",
        "            # For example, introduce a key called \"action\" with some instructions.\n",
        "            \"action\": \"Please approve/reject the tool call\",\n",
        "        })\n",
        "\n",
        "    if is_approved:\n",
        "        tool = tools_by_name[tool_call[\"name\"]]\n",
        "        return tool.invoke(tool_call)\n",
        "    else:\n",
        "        return \"Tool call rejected\"\n",
        "\n",
        "@entrypoint(checkpointer=MemorySaver())\n",
        "def agent(messages: list[BaseMessage], previous: list[BaseMessage]):\n",
        "    \"\"\" Tool calling agent \"\"\"\n",
        "    print(\"Executing agent!\")\n",
        "\n",
        "    # Add previous messages from short-term memory to the current messages\n",
        "    if previous is not None:\n",
        "        messages = add_messages(previous, messages)\n",
        "\n",
        "    # Call the LLM\n",
        "    llm_response = call_llm(messages).result()\n",
        "\n",
        "    while True:\n",
        "        if not llm_response.tool_calls:\n",
        "            break\n",
        "\n",
        "        # Execute tools\n",
        "        tool_results = [\n",
        "            call_tool(tool_call).result() for tool_call in llm_response.tool_calls\n",
        "        ]\n",
        "        messages = add_messages(messages, [llm_response, *tool_results])\n",
        "        llm_response = call_llm(messages).result()\n",
        "\n",
        "    messages = add_messages(messages, llm_response)\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOGn3M7RrasA",
        "outputId": "55757e74-188e-4768-dae7-7dc4792f6373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing agent!\n",
            "Calling LLM!\n",
            "Calling tool!\n",
            "{'tool_call': {'name': 'add', 'args': {'a': 3, 'b': 4}, 'id': 'toolu_01AFGWLbpJQ7fATZ5oLhSZUa', 'type': 'tool_call'}, 'action': 'Please approve/reject the tool call'}\n"
          ]
        }
      ],
      "source": [
        "# Thread ID\n",
        "thread_id = str(uuid.uuid4())\n",
        "\n",
        "# Config\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "# Run until the interrupt\n",
        "for item in agent.stream([HumanMessage(content=\"Add 3 and 4.\")], config, stream_mode=\"updates\"):\n",
        "    if '__interrupt__' in item:\n",
        "        print(item['__interrupt__'][0].value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "H_lma6qjrasA"
      },
      "outputs": [],
      "source": [
        "from langgraph.types import Command\n",
        "for item in agent.stream(Command(resume=True), config, stream_mode=\"updates\"):\n",
        "    if 'agent' in item:\n",
        "        item['agent'][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x8F3pTGrasB"
      },
      "source": [
        "Initial Execution Flow:\n",
        "* `call_llm` runs first and is cached\n",
        "* `call_tool` starts but hits the interrupt\n",
        "* Execution pauses waiting for human input\n",
        "\n",
        "After Resume:\n",
        "* `call_tool` continues from where it left off (since it didn't complete)\n",
        "* The tool executes if approved\n",
        "* `call_llm` runs again with the tool results\n",
        "* Final response is generated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDzsfoXJrasB"
      },
      "source": [
        "### Time travel\n",
        "\n",
        "* It can be useful to add time travel to the workflow.\n",
        "* This allows you to rewind to specific checkpoint, modify the workflow, and re-run it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3WHUCBNrasB"
      },
      "outputs": [],
      "source": [
        "@task\n",
        "def call_llm(messages: list[BaseMessage]):\n",
        "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "    return llm_with_tools.invoke(\n",
        "        [\n",
        "            SystemMessage(\n",
        "                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
        "            )\n",
        "        ]\n",
        "        + messages\n",
        "    )\n",
        "\n",
        "@task\n",
        "def call_tool(tool_call: ToolCall):\n",
        "    \"\"\"Performs the tool call\"\"\"\n",
        "    tool = tools_by_name[tool_call[\"name\"]]\n",
        "    return tool.invoke(tool_call)\n",
        "\n",
        "checkpointer = MemorySaver()\n",
        "@entrypoint(checkpointer=checkpointer)\n",
        "def agent(messages: list[BaseMessage], previous: list[BaseMessage]): # New\n",
        "    \"\"\" Tool calling agent \"\"\"\n",
        "    # Add previous messages from short-term memory to the current messages\n",
        "    if previous is not None:\n",
        "        messages = add_messages(previous, messages)\n",
        "\n",
        "    # Call the LLM\n",
        "    llm_response = call_llm(messages).result()\n",
        "\n",
        "    while True:\n",
        "        if not llm_response.tool_calls:\n",
        "            break\n",
        "\n",
        "        # Execute tools\n",
        "        tool_results = [\n",
        "            call_tool(tool_call).result() for tool_call in llm_response.tool_calls\n",
        "        ]\n",
        "        messages = add_messages(messages, [llm_response, *tool_results])\n",
        "        llm_response = call_llm(messages).result()\n",
        "\n",
        "    messages = add_messages(messages, llm_response)\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDRKrGz7rasB",
        "outputId": "50b065c5-90ed-4622-e4fc-e0dc6267023c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 3 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "[{'text': \"I'll help you add 3 and 4 using the `add` function.\", 'type': 'text'}, {'id': 'toolu_0144yacRC2U5UkwEfEzBdzm2', 'input': {'a': 3, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
            "Tool Calls:\n",
            "  add (toolu_0144yacRC2U5UkwEfEzBdzm2)\n",
            " Call ID: toolu_0144yacRC2U5UkwEfEzBdzm2\n",
            "  Args:\n",
            "    a: 3\n",
            "    b: 4\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The sum of 3 and 4 is 7.\n"
          ]
        }
      ],
      "source": [
        "# Thread ID\n",
        "thread_id = str(uuid.uuid4())\n",
        "\n",
        "# Config\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "# Run with checkpointer to persist state in memory\n",
        "messages = agent.invoke([HumanMessage(content=\"Add 3 and 4.\")], config)\n",
        "for m in messages:\n",
        "    m.pretty_print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdIic7x3rasB",
        "outputId": "ff833be1-8abc-4f18-f26d-4800abcf323a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "7 multiplied by 2 equals 14.\n"
          ]
        }
      ],
      "source": [
        "# Second turn\n",
        "for item in agent.stream([HumanMessage(content=\"Take the result and multiply it by 2.\")], config, stream_mode=\"updates\"):\n",
        "    if 'agent' in item:\n",
        "        item['agent'][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0dD-thkrasB",
        "outputId": "9d620f9b-aafb-451e-dd95-2778232d932a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': 'c0336c2c-1a97-4b6d-826c-21c33eb47ccb',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1efddbc1-103d-66d4-8001-48133c20ef14'}}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fork and do alternative second turn\n",
        "to_fork_from = list(agent.get_state_history(config))[1].config\n",
        "to_fork_from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTYoCyURrasC",
        "outputId": "b9772e2c-5314-4493-e444-fabcc4f22289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 3 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "[{'text': \"I'll help you add 3 and 4 using the `add` function.\", 'type': 'text'}, {'id': 'toolu_0144yacRC2U5UkwEfEzBdzm2', 'input': {'a': 3, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
            "Tool Calls:\n",
            "  add (toolu_0144yacRC2U5UkwEfEzBdzm2)\n",
            " Call ID: toolu_0144yacRC2U5UkwEfEzBdzm2\n",
            "  Args:\n",
            "    a: 3\n",
            "    b: 4\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The sum of 3 and 4 is 7.\n"
          ]
        }
      ],
      "source": [
        "# Get the last checkpoint, which contains the full message history\n",
        "agent_state = agent.get_state(to_fork_from)\n",
        "for m in agent_state.values:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdwOV7wprasC",
        "outputId": "bd80d269-1649-4811-8b72-0c8f0669ccca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "7 multiplied by 3 equals 21.\n"
          ]
        }
      ],
      "source": [
        "# Re-run the workflow from the fork\n",
        "for item in agent.stream([HumanMessage(content=\"Take the result and multiply it by 3.\")], to_fork_from, stream_mode=\"updates\"):\n",
        "    if 'agent' in item:\n",
        "        item['agent'][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPv3mgIkrasC"
      },
      "source": [
        "### Agent with HITL and Long-term memory (across threads)\n",
        "\n",
        "* Add interrupt to the workflow to allow for HITL\n",
        "* Add tool for [long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/#long-term-memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0Juh_-KrasC"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from typing import Annotated, Optional\n",
        "\n",
        "from langchain_core.tools import InjectedToolArg\n",
        "from langgraph.store.base import BaseStore\n",
        "\n",
        "@tool\n",
        "def upsert_memory(\n",
        "    content: str,\n",
        "    *,\n",
        "    memory_id: Optional[uuid.UUID] = None,\n",
        "    # Hide these arguments from the model.\n",
        "    store: Annotated[BaseStore, InjectedToolArg],\n",
        "):\n",
        "    \"\"\"Upsert a memory in the database.\n",
        "\n",
        "    If a memory conflicts with an existing one, then just UPDATE the\n",
        "    existing one by passing in memory_id - don't create two memories\n",
        "    that are the same. If the user corrects a memory, UPDATE it.\n",
        "\n",
        "    Args:\n",
        "        content: The main content of the memory. For example:\n",
        "            \"User expressed interest in learning about French.\"\n",
        "        memory_id: ONLY PROVIDE IF UPDATING AN EXISTING MEMORY.\n",
        "        The memory to overwrite.\n",
        "    \"\"\"\n",
        "    mem_id = memory_id or uuid.uuid4()\n",
        "\n",
        "    # BaseStore is a LangGraph persistence layer\n",
        "    store.put(\n",
        "        (\"memories\",\"lance\"),\n",
        "        key=str(mem_id),\n",
        "        value={\"content\": content},\n",
        "    )\n",
        "    return f\"Stored memory {mem_id}\"\n",
        "\n",
        "# Augment the LLM with tools\n",
        "tools = [upsert_memory]\n",
        "tools_by_name = {tool.name: tool for tool in tools}\n",
        "llm_with_memory_tool = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5bHqyUCrasC"
      },
      "outputs": [],
      "source": [
        "from langgraph.store.memory import InMemoryStore # New\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "@task\n",
        "def call_llm(messages: list[BaseMessage]):\n",
        "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "    return llm_with_memory_tool.invoke( # New\n",
        "        [\n",
        "            SystemMessage(\n",
        "                content=\"You are a helpful assistant tasked with storing memories.\" # New\n",
        "            )\n",
        "        ]\n",
        "        + messages\n",
        "    )\n",
        "\n",
        "@task\n",
        "def call_tool(tool_call: ToolCall, store: BaseStore):\n",
        "\n",
        "    # Interrupt the workflow to get a review from a human.\n",
        "    is_approved = interrupt({ # New\n",
        "            # Any json-serializable payload provided to interrupt as argument.\n",
        "            # It will be surfaced on the client side as an Interrupt when streaming data\n",
        "            # from the workflow.\n",
        "            \"tool_call\": tool_call, # The tool call we want reviewed.\n",
        "            # We can add any additional information that we need.\n",
        "            # For example, introduce a key called \"action\" with some instructions.\n",
        "            \"action\": \"Please approve/reject the tool call\",\n",
        "        })\n",
        "\n",
        "    if is_approved:\n",
        "\n",
        "        print(\"Tool call approved, Memory Added!\")\n",
        "\n",
        "        tool = tools_by_name[tool_call[\"name\"]]\n",
        "        tool.invoke({**tool_call[\"args\"], \"store\": store})\n",
        "\n",
        "        # Tool message provides confirmation to the model that the actions it took were completed\n",
        "        results = ToolMessage(content=tool_call[\"args\"][\"content\"], tool_call_id=tool_call[\"id\"])\n",
        "        return results\n",
        "    else:\n",
        "        return \"Tool call rejected\"\n",
        "\n",
        "in_memory_store = InMemoryStore()\n",
        "@entrypoint(checkpointer=MemorySaver(), store=in_memory_store)\n",
        "def agent(messages: list[BaseMessage], previous: list[BaseMessage], store: BaseStore):\n",
        "    \"\"\" Tool calling agent \"\"\"\n",
        "\n",
        "    # Add previous messages from short-term memory to the current messages\n",
        "    if previous is not None:\n",
        "        messages = add_messages(previous, messages)\n",
        "\n",
        "    # New\n",
        "    # Retrieve the most recent memories for context\n",
        "    memories = store.search(\n",
        "        (\"memories\",\"lance\"),\n",
        "        limit=10,\n",
        "    )\n",
        "\n",
        "    # New\n",
        "    # Format memories for inclusion in the prompt\n",
        "    formatted = \"\\n\".join(f\"[{mem.key}]: {mem.value} (similarity: {mem.score})\" for mem in memories)\n",
        "    if formatted:\n",
        "        formatted = f\"\"\"\n",
        "<memories>\n",
        "{formatted}\n",
        "</memories>\"\"\"\n",
        "\n",
        "    # New\n",
        "    # Call the LLM\n",
        "    llm_response = call_llm([SystemMessage(content=f\"Here is some context for you about the user: {formatted}\"), *messages]).result()\n",
        "\n",
        "    while True:\n",
        "        if not llm_response.tool_calls:\n",
        "            break\n",
        "\n",
        "        # Execute tools\n",
        "        tool_results = [\n",
        "            call_tool(tool_call=tool_call, store=store).result() for tool_call in llm_response.tool_calls\n",
        "        ]\n",
        "        messages = add_messages(messages, [llm_response, *tool_results])\n",
        "        llm_response = call_llm(messages).result()\n",
        "\n",
        "    messages = add_messages(messages, llm_response)\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQSTX3oGrasC",
        "outputId": "1f1d3ca7-708b-42a5-bd57-9b6dce83067e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'tool_call': {'name': 'upsert_memory', 'args': {'content': \"User's name is Lance.\"}, 'id': 'toolu_01Y1J6BnLWejpoQfF72GWP9L', 'type': 'tool_call'}, 'action': 'Please approve/reject the tool call'}\n"
          ]
        }
      ],
      "source": [
        "# Thread ID\n",
        "thread_id = str(uuid.uuid4())\n",
        "\n",
        "# Config\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "# Run until the interrupt\n",
        "for item in agent.stream([HumanMessage(content=\"Hi my name is Lance.\")], config, stream_mode=\"updates\"):\n",
        "    if '__interrupt__' in item:\n",
        "        print(item['__interrupt__'][0].value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ9V5WVnrasD",
        "outputId": "30168f61-e9b5-4abf-d196-1168b173f81c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool call approved, Memory Added!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Nice to meet you Lance! Is there anything else you'd like me to help you with?\n"
          ]
        }
      ],
      "source": [
        "for item in agent.stream(Command(resume=True), config, stream_mode=\"updates\"):\n",
        "    if 'agent' in item:\n",
        "        item['agent'][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPt1NZcjrasD",
        "outputId": "37905a62-0a12-4874-df57-119f1cac3ea9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Item(namespace=['memories', 'lance'], key='20544090-a509-46ac-85f2-04b3f54db0d2', value={'content': \"User's name is Lance.\"}, created_at='2025-01-28T21:41:25.959474+00:00', updated_at='2025-01-28T21:41:25.959475+00:00', score=None)]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "in_memory_store.search((\"memories\",\"lance\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8uQC9xhrasD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OVJZpKarasD"
      },
      "source": [
        "### Overview\n",
        "\n",
        "- Vanilla agent\n",
        "- Added short-term memory (w/ tracing)\n",
        "- Added short-term memory + HITL (w/ tracing)\n",
        "- Added short-term memory + HITL + time travel (w/ tracing)\n",
        "- Added short-term memory + HITL + long-term memory (w/ tracing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y3CSwo3rasD"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "test-workflow-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}